package spacy

import (
	"strings"
	"testing"
)

// Test edge cases with unusual inputs
func TestEdgeCases(t *testing.T) {
	nlp, err := NewNLP("en_core_web_sm")
	if err != nil {
		t.Fatalf("Failed to create NLP: %v", err)
	}
	defer nlp.Close()

	tests := []struct {
		name     string
		text     string
		testFunc func(t *testing.T, nlp *NLP, text string)
	}{
		{
			name: "Very long single word",
			text: strings.Repeat("a", 10000),
			testFunc: func(t *testing.T, nlp *NLP, text string) {
				tokens := nlp.Tokenize(text)
				if len(tokens) == 0 {
					t.Error("Failed to tokenize very long word")
				}
			},
		},
		{
			name: "Only whitespace",
			text: "   \t\n\r   ",
			testFunc: func(t *testing.T, nlp *NLP, text string) {
				tokens := nlp.Tokenize(text)
				// Spacy might return empty or whitespace tokens
				t.Logf("Whitespace tokenization returned %d tokens", len(tokens))
			},
		},
		{
			name: "Unicode emojis",
			text: "Hello üòÄ World üåç Test üöÄ",
			testFunc: func(t *testing.T, nlp *NLP, text string) {
				tokens := nlp.Tokenize(text)
				if len(tokens) < 3 {
					t.Error("Failed to handle emoji text properly")
				}
			},
		},
		{
			name: "Mixed scripts",
			text: "English —Ä—É—Å—Å–∫–∏–π ‰∏≠Êñá ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ‡§π‡§ø‡§®‡•ç‡§¶‡•Ä",
			testFunc: func(t *testing.T, nlp *NLP, text string) {
				tokens := nlp.Tokenize(text)
				if len(tokens) < 5 {
					t.Error("Failed to handle mixed scripts")
				}
			},
		},
		{
			name: "HTML tags",
			text: "<p>Hello <b>World</b></p>",
			testFunc: func(t *testing.T, nlp *NLP, text string) {
				tokens := nlp.Tokenize(text)
				entities := nlp.ExtractEntities(text)
				t.Logf("HTML text produced %d tokens and %d entities", len(tokens), len(entities))
			},
		},
		{
			name: "Nested parentheses",
			text: "This (is (a (nested) test) example) here.",
			testFunc: func(t *testing.T, nlp *NLP, text string) {
				sentences := nlp.SplitSentences(text)
				if len(sentences) != 1 {
					t.Errorf("Expected 1 sentence, got %d", len(sentences))
				}
			},
		},
		{
			name: "Special characters",
			text: "Test @#$%^&*()_+-=[]{}|;':\",./<>?",
			testFunc: func(t *testing.T, nlp *NLP, text string) {
				tokens := nlp.Tokenize(text)
				if len(tokens) == 0 {
					t.Error("Failed to tokenize special characters")
				}
			},
		},
		{
			name: "Repeated punctuation",
			text: "What???? Really!!!! No way......",
			testFunc: func(t *testing.T, nlp *NLP, text string) {
				sentences := nlp.SplitSentences(text)
				tokens := nlp.Tokenize(text)
				if len(sentences) == 0 || len(tokens) == 0 {
					t.Error("Failed to handle repeated punctuation")
				}
			},
		},
		{
			name: "Line breaks",
			text: "First line\nSecond line\rThird line\r\nFourth line",
			testFunc: func(t *testing.T, nlp *NLP, text string) {
				sentences := nlp.SplitSentences(text)
				if len(sentences) < 2 {
					t.Logf("Line breaks produced %d sentences", len(sentences))
				}
			},
		},
		{
			name: "Zero-width characters",
			text: "Hello\u200bWorld\u200cTest\u200d",
			testFunc: func(t *testing.T, nlp *NLP, text string) {
				tokens := nlp.Tokenize(text)
				t.Logf("Zero-width chars produced %d tokens", len(tokens))
			},
		},
		{
			name: "Moderately long text",
			text: strings.Repeat("This is a test sentence. ", 100),
			testFunc: func(t *testing.T, nlp *NLP, text string) {
				tokens := nlp.Tokenize(text)
				if len(tokens) < 500 {
					t.Errorf("Expected many tokens for long text, got %d", len(tokens))
				}
				sentences := nlp.SplitSentences(text)
				if len(sentences) < 90 {
					t.Errorf("Expected many sentences, got %d", len(sentences))
				}
			},
		},
		{
			name: "ASCII text only",
			text: "Hello World Test 123",
			testFunc: func(t *testing.T, nlp *NLP, text string) {
				tokens := nlp.Tokenize(text)
				if len(tokens) < 4 {
					t.Errorf("Expected at least 4 tokens, got %d", len(tokens))
				}
			},
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			tt.testFunc(t, nlp, tt.text)
		})
	}
}

// Test malformed inputs - Simplified to avoid Python UTF-8 issues
func TestMalformedInputs(t *testing.T) {
	nlp, err := NewNLP("en_core_web_sm")
	if err != nil {
		t.Fatalf("Failed to create NLP: %v", err)
	}
	defer nlp.Close()

	tests := []struct {
		name string
		text string
	}{
		{"Empty string", ""},
		{"Single null character", "Hello\x00World"}, // Single null byte
		{"Tab and newline", "Hello\t\nWorld"},
		{"Special ASCII", "Hello!@#$%^&*()World"},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			// These should not panic, even if they return empty results
			func() {
				defer func() {
					if r := recover(); r != nil {
						t.Errorf("Panic on malformed input: %v", r)
					}
				}()

				tokens := nlp.Tokenize(tt.text)
				_ = nlp.ExtractEntities(tt.text)
				_ = nlp.SplitSentences(tt.text)
				_ = nlp.POSTag(tt.text)
				_ = nlp.GetDependencies(tt.text)
				_ = nlp.GetLemmas(tt.text)

				t.Logf("Handled input: got %d tokens", len(tokens))
			}()
		})
	}
}

// Test boundary conditions
func TestBoundaryConditions(t *testing.T) {
	nlp, err := NewNLP("en_core_web_sm")
	if err != nil {
		t.Fatalf("Failed to create NLP: %v", err)
	}
	defer nlp.Close()

	tests := []struct {
		name string
		text string
	}{
		{"Single character", "a"},
		{"Single digit", "1"},
		{"Single punctuation", "."},
		{"Two characters", "ab"},
		{"Maximum reasonable length", strings.Repeat("a ", 1000)},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			tokens := nlp.Tokenize(tt.text)
			entities := nlp.ExtractEntities(tt.text)
			sentences := nlp.SplitSentences(tt.text)
			posMap := nlp.POSTag(tt.text)
			depMap := nlp.GetDependencies(tt.text)
			lemmaMap := nlp.GetLemmas(tt.text)

			// Just ensure these don't crash
			t.Logf("Boundary test '%s': %d tokens, %d entities, %d sentences, %d POS, %d deps, %d lemmas",
				tt.name, len(tokens), len(entities), len(sentences), len(posMap), len(depMap), len(lemmaMap))
		})
	}
}

// Test consistency across multiple calls
func TestConsistency(t *testing.T) {
	nlp, err := NewNLP("en_core_web_sm")
	if err != nil {
		t.Fatalf("Failed to create NLP: %v", err)
	}
	defer nlp.Close()

	text := "The quick brown fox jumps over the lazy dog."

	// Call each function multiple times and check for consistency
	for i := 0; i < 10; i++ {
		tokens1 := nlp.Tokenize(text)
		tokens2 := nlp.Tokenize(text)

		if len(tokens1) != len(tokens2) {
			t.Errorf("Inconsistent tokenization: %d vs %d tokens", len(tokens1), len(tokens2))
		}

		entities1 := nlp.ExtractEntities(text)
		entities2 := nlp.ExtractEntities(text)

		if len(entities1) != len(entities2) {
			t.Errorf("Inconsistent NER: %d vs %d entities", len(entities1), len(entities2))
		}

		sentences1 := nlp.SplitSentences(text)
		sentences2 := nlp.SplitSentences(text)

		if len(sentences1) != len(sentences2) {
			t.Errorf("Inconsistent sentence splitting: %d vs %d sentences", len(sentences1), len(sentences2))
		}
	}
}

// Test numeric edge cases
func TestNumericEdgeCases(t *testing.T) {
	nlp, err := NewNLP("en_core_web_sm")
	if err != nil {
		t.Fatalf("Failed to create NLP: %v", err)
	}
	defer nlp.Close()

	tests := []struct {
		name string
		text string
	}{
		{"Large number", "999999999999999999999999999999"},
		{"Decimal", "3.14159265358979323846"},
		{"Scientific notation", "1.23e-45"},
		{"Negative number", "-123456789"},
		{"Binary", "0b101010"},
		{"Hexadecimal", "0xDEADBEEF"},
		{"Roman numerals", "MMXXIV"},
		{"Fractions", "1/2 3/4 5/6"},
		{"Mixed numbers", "1 1/2 cups"},
		{"Phone numbers", "+1-555-123-4567"},
		{"IP addresses", "192.168.1.1"},
		{"Dates", "12/31/2024"},
		{"Times", "23:59:59"},
		{"Currency", "$1,234,567.89"},
		{"Percentages", "99.99%"},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			tokens := nlp.Tokenize(tt.text)
			entities := nlp.ExtractEntities(tt.text)

			if len(tokens) == 0 {
				t.Errorf("Failed to tokenize numeric text: %s", tt.text)
			}

			t.Logf("Numeric text '%s': %d tokens, %d entities", tt.name, len(tokens), len(entities))
		})
	}
}
